{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kFold train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디렉토리 경로\n",
    "import os\n",
    "\n",
    "# 데이터셋 관련\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "# 학습 관련\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "import detectron2.data.transforms as T\n",
    "\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_train_loader, build_detection_test_loader\n",
    "from detectron2.data import detection_utils as utils\n",
    "\n",
    "# 기타\n",
    "import wandb\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "\n",
    "# KFold\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이터 등록"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 리소스 및 경로 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리소스\n",
    "coco_dataset_train = 'coco_trash_train'\n",
    "coco_dataset_test = 'coco_trash_test'\n",
    "\n",
    "coco_fold_train = 'coco_fold_train'\n",
    "coco_fold_test = 'coco_fold_test'\n",
    "\n",
    "# 경로\n",
    "path_dataset = '/data/ephemeral/home/dataset/'\n",
    "\n",
    "path_output = './output_fold'\n",
    "path_output_eval = './output_eval'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *수정이 필요한 리소스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_pretrained = 'COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml'\n",
    "backbone = 'build_resnet_fpn_backbone'\n",
    "\n",
    "model_title = path_model_pretrained.split(\"/\")[1].split(\".\")[0]\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-fold json 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kfold_datasets(k, coco_data, output_dir):\n",
    "    \n",
    "    image_to_annotations = {}\n",
    "\n",
    "    for anno in coco_data['annotations']:\n",
    "        image_id = anno['image_id']\n",
    "\n",
    "        if image_id not in image_to_annotations:\n",
    "            image_to_annotations[image_id] = []\n",
    "\n",
    "        image_to_annotations[image_id].append(anno)\n",
    "    \n",
    "    image_ids = list(image_to_annotations.keys())\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=22)\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(image_ids)):\n",
    "        train_image_ids = [image_ids[i] for i in train_idx]\n",
    "        val_image_ids = [image_ids[i] for i in val_idx]\n",
    "\n",
    "        # Train/Val annotation, 이미지 필터링\n",
    "        train_annotations = [anno for image_id in train_image_ids for anno in image_to_annotations[image_id]]\n",
    "        val_annotations = [anno for image_id in val_image_ids for anno in image_to_annotations[image_id]]\n",
    "\n",
    "        train_images = [img for img in coco_data['images'] if img['id'] in train_image_ids]\n",
    "        val_images = [img for img in coco_data['images'] if img['id'] in val_image_ids]\n",
    "\n",
    "        \n",
    "        # train fold\n",
    "        train_data = coco_data.copy()\n",
    "        train_data['annotations'] = train_annotations\n",
    "        train_data['images'] = train_images\n",
    "\n",
    "        with open(os.path.join(output_dir, f'train_fold_{fold_idx}.json'), 'w') as f:\n",
    "            json.dump(train_data, f)\n",
    "\n",
    "\n",
    "        # validation fold\n",
    "        val_data = coco_data.copy()\n",
    "        val_data['annotations'] = val_annotations\n",
    "        val_data['images'] = val_images\n",
    "        \n",
    "        with open(os.path.join(output_dir, f'val_fold_{fold_idx}.json'), 'w') as f:\n",
    "            json.dump(val_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터셋 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_kfold_datasets(k, path_dataset):\n",
    "    for fold_idx in range(k):\n",
    "        train_dataset_name = f'{coco_fold_train}{fold_idx}'\n",
    "        val_dataset_name = f'{coco_fold_test}{fold_idx}'\n",
    "\n",
    "        train_json = os.path.join(path_dataset, f'train_fold_{fold_idx}.json')\n",
    "        val_json = os.path.join(path_dataset, f'val_fold_{fold_idx}.json')\n",
    "\n",
    "        if train_dataset_name not in DatasetCatalog.list():\n",
    "            register_coco_instances(train_dataset_name, {}, train_json, path_dataset)\n",
    "        \n",
    "        if val_dataset_name not in DatasetCatalog.list():\n",
    "            register_coco_instances(val_dataset_name, {}, val_json, path_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 초기 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기본 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "wandb.init(project=\"2024 부스트캠프 재활용품 분류대회(22, CSV)\", \n",
    "           name=f'{model_title} {datetime.now().strftime(\"%m-%d %H:%M\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "\n",
    "# COCO 사전 학습 가중치 경로 설정\n",
    "cfg.merge_from_file(model_zoo.get_config_file(path_model_pretrained))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(path_model_pretrained)\n",
    "\n",
    "cfg.SEED = 22\n",
    "cfg.DATALOADER.NUM_WOREKRS = 4\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 10                     # 분류할 클래스 수\n",
    "cfg.TEST.EVAL_PERIOD = 500                               # 검증 주기(단위 : iter)\n",
    "\n",
    "schedulers = ['WarmupMultiStepLR',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.SOLVER.IMS_PER_BATCH = 4                              # 배치크기\n",
    "cfg.SOLVER.BASE_LR = 0.0005                               # 초기 학습률\n",
    "cfg.SOLVER.MAX_ITER = 20000                               # 최대 학습 반복 수\n",
    "cfg.SOLVER.STEPS = (cfg.SOLVER.MAX_ITER // 2, \n",
    "                    cfg.SOLVER.MAX_ITER * 2 //3)          # 학습률 감소 단계(50%, 75%) 권장\n",
    "\n",
    "cfg.SOLVER.GAMMA = 0.05                                   # 학습률 감소 비율(%)\n",
    "cfg.SOLVER.AMP.ENABLED = True\n",
    "cfg.SOLVER.LR_SCHEDULER_NAME = schedulers[0]\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128            # 각 이미지당 샘플링할 RoI 개수\n",
    "\n",
    "cfg.MODEL.RPN.NMS_THRESH = 0.7\n",
    "\n",
    "cfg.MODEL.BACKBONE.NAME = backbone\n",
    "cfg.MODEL.FPN.IN_FEATURES = [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
    "cfg.MODEL.RPN.IN_FEATURES = [\"p2\", \"p3\", \"p4\", \"p5\"]\n",
    "\n",
    "\n",
    "cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[16, 32, 64, 128, 256, 512]]\n",
    "# cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.5, 1.0, 2.0]]\n",
    "# cfg.MODEL.ANCHOR_GENERATOR.ANGLES = [[-90, 0, 90]]\n",
    "\n",
    "\n",
    "aug_list = [                                            # 데이터 증강 옵션\n",
    "    T.RandomFlip(prob=0.5, horizontal=False, vertical=True),\n",
    "    T.RandomBrightness(0.8, 1.8),\n",
    "    T.RandomContrast(0.6, 1.3),\n",
    "    T.RandomRotation(angle=[-15, 15]),\n",
    "    T.RandomCrop(crop_type=\"relative_range\", crop_size=(0.8, 0.8)),\n",
    "    T.RandomLighting(0.1),\n",
    "]\n",
    "\n",
    "config = {\n",
    "    \"model\": model_title,\n",
    "    \"backbone\": backbone,\n",
    "    \"roi size\": cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE,\n",
    "    \"learning rate\": cfg.SOLVER.BASE_LR,\n",
    "    \"scheduler\": cfg.SOLVER.LR_SCHEDULER_NAME,\n",
    "    \"augmentation\": aug_list,\n",
    "}\n",
    "wandb.config.update(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전처리 및 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyMapper(dataset_dict):\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)\n",
    "    image = utils.read_image(dataset_dict['file_name'], format='BGR')\n",
    "    \n",
    "    transform_list = aug_list\n",
    "\n",
    "    image, transforms = T.apply_transform_gens(transform_list, image)\n",
    "    \n",
    "    dataset_dict['image'] = torch.as_tensor(image.transpose(2,0,1).astype('float32'))\n",
    "    \n",
    "    annos = [\n",
    "        utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n",
    "        for obj in dataset_dict.pop('annotations')\n",
    "        if obj.get('iscrowd', 0) == 0\n",
    "    ]\n",
    "    \n",
    "    instances = utils.annotations_to_instances(annos, image.shape[:2])\n",
    "    dataset_dict['instances'] = utils.filter_empty_instances(instances)\n",
    "    \n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(DefaultTrainer):\n",
    "    \n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return build_detection_train_loader(cfg, mapper=MyMapper)\n",
    "    \n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n",
    "    \n",
    "    \n",
    "    # Wandb 기록용\n",
    "    def log_metrics(self, results):\n",
    "        wandb.log({\n",
    "            \"bbox_AP\": results[\"bbox\"][\"AP\"],\n",
    "            \"bbox_AP50\": results[\"bbox\"][\"AP50\"],\n",
    "            \"bbox_AP75\": results[\"bbox\"][\"AP75\"],\n",
    "            \"bbox_IoU\": results[\"bbox\"][\"AP\"],\n",
    "        })\n",
    "\n",
    "\n",
    "    # Wandb 기록용\n",
    "    def evaluate_model(self):\n",
    "        evaluator = COCOEvaluator(coco_dataset_test, self.cfg, False, output_dir=path_output_eval)\n",
    "        val_loader = build_detection_test_loader(self.cfg, coco_dataset_test)\n",
    "        results = inference_on_dataset(self.build_model(self.cfg), val_loader, evaluator)\n",
    "        self.log_metrics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_training(k, path_dataset, cfg):\n",
    "    for fold_idx in range(k):\n",
    "        cfg.DATASETS.TRAIN = (f'{coco_fold_train}{fold_idx}',)\n",
    "        cfg.DATASETS.TEST = (f'{coco_fold_test}{fold_idx}',)\n",
    "        \n",
    "        cfg.OUTPUT_DIR = f'{path_output}_{fold_idx}'\n",
    "        os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "        trainer = MyTrainer(cfg)\n",
    "        trainer.resume_or_load(resume=False)\n",
    "        trainer.train()\n",
    "\n",
    "        trainer.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_dataset, 'train.json'), 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "create_kfold_datasets(k, coco_data, path_dataset)\n",
    "register_kfold_datasets(k, path_dataset)\n",
    "kfold_training(k, path_dataset, cfg)\n",
    "\n",
    "wandb.finish"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
